name: Data Collection (Nightly)

# This workflow is intended for the project maintainer only
# It scrapes fresh NBA data and uploads to Kaggle
# Requires secrets: KAGGLE_USERNAME, KAGGLE_KEY, PROXY_URL

on:
  schedule:
    - cron: '0 8 * * *'  # 3am EST = 8am UTC
  workflow_dispatch:  # Manual trigger

jobs:
  collect_and_publish:
    runs-on: ubuntu-latest
    # Only run on original repo, not forks (prevents errors from missing secrets)
    if: github.repository == 'cmunch1/nba_analysis_project'

    # Use the pre-built Docker image
    container:
      image: ghcr.io/${{ github.repository }}:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
      volumes:
        - /home/runner/work/${{ github.repository }}/data:/app/data

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Kaggle credentials
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          mkdir -p ~/.kaggle
          echo "{\"username\":\"$KAGGLE_USERNAME\",\"key\":\"$KAGGLE_KEY\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json

      - name: Download Previous Data from Kaggle
        run: |
          mkdir -p data/cumulative_scraped data/processed

          # Download existing dataset (or start fresh if first run)
          kaggle datasets download -d chrismunch/nba-game-team-statistics \
            -p data --unzip || echo "First run - no existing data"

          # Move downloaded files to correct locations
          if [ -d "data/cumulative_scraped" ]; then
            echo "Previous cumulative data downloaded"
          fi

      - name: Run Webscraping
        run: python -m src.nba_app.webscraping.main
        env:
          PROXY_URL: ${{ secrets.PROXY_URL }}

      - name: Run Data Processing
        run: python -m src.nba_app.data_processing.main

      - name: Upload to Kaggle
        run: ./scripts/upload_to_kaggle.sh

      - name: Upload logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: pipeline-logs
          path: logs/
          retention-days: 7
