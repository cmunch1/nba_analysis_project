# settings for experiment logging
experiment_name: "NBA Predictor"
experiment_description: "NBA Predictor"



#sort columns - ensures that the columns are in the correct order - important for TimeSeriesSplit cross validation
sort_columns:
  - season
  - season_progress

# sort order: ascending = true, descending = false
sort_order:
  - true
  - true

# primary id for the dataframe - not used for modeling, but useful for analyzing predictions
primary_id_column: game_id

# these columns are not useful for modeling - just informational text that is already encoded elsewhere
# they will be dropped from the dataframe right before modeling
non_useful_columns:
  - h_team
  - h_match_up
  - v_team
  - v_match_up
  - game_date  # Date column - temporal info already captured in season/season_progress

# indicates which columns are categorical so they can be encoded and identified by the model
categorical_features:
  - h_team_id
  - v_team_id

# this allows you to skip preprocessing for all models without having to disable all the settings in the preprocessing_config.yaml file
# the individual settings are in the preprocessing_config.yaml file
perform_preprocessing: false

# hyperparameters path (${PROJECT_ROOT} is the root directory of the project and is parsed from the environment variable in config.py)
current_hyperparameters: "${PROJECT_ROOT}/configs/hyperparameters.json"

# hyperparameters storage directory
hyperparameter_history_dir: "${PROJECT_ROOT}/ml_artifacts/experiments/hyperparameters"

# hyperparameter metadata - fields that are not used as hyperparameters but may be included in tracking
hyperparameter_metadata:
  - name
  - metrics
  - updated_at
  - experiment_id
  - run_id
  - num_boost_round
  - early_stopping
  - enable_categorical
  - categorical_features


# model testing options
perform_oof_cross_validation: true
perform_validation_set_testing: true
save_oof_predictions: false # save the predictions for the out of fold cross validation to a csv file
save_validation_predictions: true # save the predictions for the validation set to a csv file
log_experiment: true


# Cross validation settings (only TimeSeriesSplit or StratifiedKFold are supported currently)
cross_validation_type: TimeSeriesSplit
n_splits: 5
random_state: 42

# models to use in this experiment run
models:
  xgboost: true
  lightgbm: false
  catboost: false
  sklearn_randomforest: false
  sklearn_logisticregression: false
  sklearn_histgradientboosting: false
  pytorch: false


# learning curve settings
generate_learning_curve_data: true 

# SHAP settings
calculate_shap_values: true
max_shap_interaction_memory_gb: 256.0
calculate_shap_interactions: false

# Visualization output settings
charts_output_dir: "${PROJECT_ROOT}/src/ml_framework/ml_artifacts/visualizations/charts" # Base directory for chart outputs

# Postprocessing settings
postprocessing:
  # Probability calibration
  enable_calibration: true # Enable probability calibration
  calibration_method: sigmoid # 'sigmoid' (Platt scaling) or 'isotonic' (isotonic regression)
  calibration_cv_folds: 5 # Number of CV folds (use 'prefit' for already trained models)

  # Calibration evaluation
  save_calibration_curves: true # Save calibration curve plots
  save_calibration_metrics: true # Save calibration metrics (Brier score, ECE, log loss)
  calibration_curve_bins: 10 # Number of bins for calibration curves

  # Model registry integration
  save_calibrator_to_registry: true # Save calibration artifact with model

# Feature audit settings
generate_feature_audit: false # Enable feature audit generation
save_feature_audit: false # Save audit results to CSV/parquet
feature_audit_format: csv # Output format: 'csv' or 'parquet' (parquet preserves precision)

# Feature audit metrics configuration
feature_audit_metrics:
  # Statistical metrics (always computed)
  compute_basic_stats: true # coverage, missing_rate, unique_values, variance, cardinality

  # Importance metrics (reused from training results when available)
  compute_shap_importance: true # Requires calculate_shap_values: true
  compute_permutation_importance: true # Computed on validation set
  permutation_n_repeats: 5 # Number of permutation repeats

  # Redundancy detection
  compute_correlation_matrix: true # Pairwise feature correlations
  compute_vif: false # Variance Inflation Factor (expensive for high-dimensional data)
  correlation_method: pearson # 'pearson', 'spearman', 'kendall'

  # Stability scoring (across CV folds)
  compute_stability_score: true # Requires perform_oof_cross_validation: true
  stability_top_k: 20 # Number of top features to track for stability

  # Target correlation
  compute_target_correlation: true # Correlation between features and target

# Feature audit thresholds (for drop_candidate_score)
feature_audit_thresholds:
  near_zero_importance_percentile: 10 # Bottom X% of features by SHAP
  near_zero_importance_threshold: 0.0 # Absolute permutation importance threshold
  high_missing_threshold: 0.4 # Flag features with >40% missing values
  high_collinearity_threshold: 0.95 # Flag features with >0.95 pairwise correlation
  low_stability_threshold: 0.2 # Flag features appearing in <20% of top-k across folds
  drop_candidate_threshold: 2 # Minimum score to flag as drop candidate

# Feature audit output
feature_audit_output_dir: "${PROJECT_ROOT}/ml_artifacts/features/audits" # Directory for audit files
feature_audit_versioning: true # Include run_id and timestamp in filename

# Feature pruning settings
enable_feature_pruning: false # Master switch for automated pruning
feature_pruning:
  # Pruning thresholds
  drop_candidate_threshold: 2 # Minimum drop_candidate_score to prune

  # Performance constraints
  min_acceptable_auc_delta: -0.01 # Allow up to 1% AUC drop
  min_acceptable_accuracy_delta: -0.02 # Allow up to 2% accuracy drop

  # Safety checks
  min_features_to_keep: 10 # Never prune below this many features
  max_features_to_drop_pct: 0.8 # Never drop more than 80% of features

  # Output
  save_pruned_datasets: false # Save pruned train/val CSV to data/pruned/
  save_comparison_report: true # Save comparison report
  comparison_report_dir: "${PROJECT_ROOT}/ml_artifacts/features/pruning/reports" # Report output directory






